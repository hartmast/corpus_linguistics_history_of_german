# Matrices
matrix(c(1,2,3,4,5,6), ncol = 2)
# Matrices
matrix(c(1,2,3,4,5,6), ncol = 2)
# Matrices
my_matrix <- matrix(c(1,2,3,4,5,6), ncol = 2)
# Lists
list(my_vector, my_data_frame, my_matrix)
# Lists
my_list <- list(my_vector, my_data_frame, my_matrix)
my_list[[1]]
my_vector
my_vector[2]
my_list[[1]][2]
my_list
my_list[[2]][3,2]
my_second_data_frame <- data.frame(A = my_vector, B = my_character_vector,
C = my_other_character_vector)
# Vectors
my_vector <- c(1,2,3)
my_character_vector <- c("a", "b", "c")
my_other_character_vector <- c(1, "two", 3)
# Data frames
my_data_frame <- data.frame(names = c("Jack", "Jill", "Hensel", "Gretel"),
gingerbread = c(5, 6, 2, 10) )
my_data_frame[2,2]
my_data_frame[3,1]
rownames(my_data_frame)
my_second_data_frame <- data.frame(A = my_vector, B = my_character_vector,
C = my_other_character_vector)
# Matrices
my_matrix <- matrix(c(1,2,3,4,5,6), ncol = 2)
# Lists
my_list <- list(my_vector, my_data_frame, my_matrix)
my_vector[2]
my_list[[2]][3,2]
# Vectors
my_vector <- c(1,2,3)
my_character_vector <- c("a", "b", "c")
my_other_character_vector <- c(1, "two", 3)
# Data frames
my_data_frame <- data.frame(names = c("Jack", "Jill", "Hensel", "Gretel"),
gingerbread = c(5, 6, 2, 10) )
my_data_frame[2,2]
my_data_frame[3,1]
rownames(my_data_frame)
my_second_data_frame <- data.frame(A = my_vector, B = my_character_vector,
C = my_other_character_vector)
# Matrices
my_matrix <- matrix(c(1,2,3,4,5,6), ncol = 2)
# Lists
my_list <- list(my_vector, my_data_frame, my_matrix)
my_vector[2]
my_list[[2]][3,2]
install.packages("tidyverse")
# install.packages("tidyverse")
library(tidyverse)
?separate
library(tspredict)
library(tidyverse)
library(vroom)
library(wizard) # available at github.com/hartmast
library(lme4)
library(effects)
library(Hmisc)
library(caret)
library(rms)
library(afex)
library(scales)
library(patchwork)
library(writexl)
library(party)
library(partykit)
library(pscl)
library(glmmTMB)
# install.packages("tidyverse")
install.packages("tspredict")
# Vectors
my_vector <- c(1,2,3)
my_character_vector <- c("a", "b", "c")
my_other_character_vector <- c(1, "two", 3)
# Data frames
my_data_frame <- data.frame(names = c("Jack", "Jill", "Hensel", "Gretel"),
gingerbread = c(5, 6, 2, 10) )
my_data_frame[2,2]
my_data_frame[3,1]
rownames(my_data_frame)
my_second_data_frame <- data.frame(A = my_vector, B = my_character_vector,
C = my_other_character_vector)
# Matrices
my_matrix <- matrix(c(1,2,3,4,5,6), ncol = 2)
# Lists
my_list <- list(my_vector, my_data_frame, my_matrix)
my_vector[2]
my_list[[2]][3,2]
# install.packages("tidyverse")
library(tidyverse)
?separate
library(vroom)
d <- vroom("/Users/stefanhartmann/Downloads/ger-all/ppmi/1800-index.pkl")
d <- readLines("/Users/stefanhartmann/Downloads/ger-all/ppmi/1800-index.pkl")
head(d)
head(d, 10)
head(d, 1000)
1967+70
# Tulips: Example from McElreath (2020)
# t <- read_csv("data/tulips.csv")
t <- read.csv("https://raw.githubusercontent.com/rmcelreath/rethinking/master/data/tulips.csv")
# Tulips: Example from McElreath (2020)
# t <- read_csv("data/tulips.csv")
t <- read_delim("https://raw.githubusercontent.com/rmcelreath/rethinking/master/data/tulips.csv",
delim = ";")
library(tidyverse)
library(car)
library(visreg)
library(effects)
library(ggeffects)
library(interactions)
library(rsm)
library(moderndive)
# Tulips: Example from McElreath (2020)
# t <- read_csv("data/tulips.csv")
t <- read_delim("https://raw.githubusercontent.com/rmcelreath/rethinking/master/data/tulips.csv",
delim = ";")
# center predictors
t$water.c <- t$water - mean(t$water)
t$shade.c <- t$shade - mean(t$shade)
# Model without interaction
m00 <- lm(blooms ~ water.c + shade.c, data = t)
# model with interaction
m01 <- lm(blooms ~ water.c + shade.c + water.c : shade.c, data = t) # aequivalent zu m03 <- lm(blooms ~ water.c * shade.c, data = t)
m01b <- lm(blooms ~ water.c + shade.c + water.c:shade.c, data = t)
m02 <- lm(blooms ~ water.c*shade.c, data = t)
# compare models
summary(m00)
summary(m01)
summary(m02)
# visualize interactions
visreg(m01, "shade.c", by = "water.c")
# for comparison:
visreg(m00, "shade.c", by = "water.c") # same slope for all!
# multicollinearity?
car::vif(m00)
car::vif(m01, type = "predictor")
d <- read_csv("data/perry_winter_2017_iconicity.csv")
# d <- read_csv("data/perry_winter_2017_iconicity.csv")
d <- read_csv("https://osf.io/download/43btm/")
View(d)
# only use nouns and verbs
d <- filter(d, POS %in% c("Verb", "Noun"))
# explorative visualization
ggplot(d, aes(x = SER, y = Iconicity, col = POS, pch = POS)) +
geom_point() +
scale_color_viridis_d(begin = .1, end = .9) + # cosmetics
theme_bw() +                                  # cosmetics
guides(col = guide_legend(reverse = T),       # cosmetics
pch = guide_legend(reverse = T))       # cosmetics
# centering predictors
d$SER_c <- d$SER - mean(d$SER, na.rm = T)
# plot again with centered predictor:
(p <- ggplot(d, aes(x = SER_c, y = Iconicity, col = POS, pch = POS)) +
geom_point(alpha=.3) +
scale_color_viridis_d(begin = .1, end = .9) + # cosmetics
theme_bw() +                                  # cosmetics
guides(col = guide_legend(reverse = T),       # cosmetics
pch = guide_legend(reverse = T)))      # cosmetics
p + geom_smooth()
# first, model without interaction:
nv_m1 <- lm(Iconicity ~ SER_c + POS, data = d)
summary(nv_m1)
# visualize parallel slopes using the moderndive package:
p + geom_parallel_slopes()
p + geom_smooth(method = "lm", se = FALSE)
# the following two commands are equivalent:
nv_m2 <- lm(Iconicity ~ SER_c * POS, data = d)
nv_m2 <- lm(Iconicity ~ SER_c + POS + SER_c:POS, data = d) # same thing
summary(nv_m2)
plot(allEffects(nv_m1)) # same slope for both
plot(allEffects(nv_m2)) # different slopes for nouns and verbs
# Interpreting interactions
coef(nv_m2)
# Important: the SER slope is now the slope of sensory
# experience ratings ONLY FOR THE NOUNS.
# Compare what happens when we fit a model
# only with the nouns
lm(Iconicity ~ SER_c, data = filter(d, POS == "Noun")) %>% coef
# Likewise, the POSVerb effect is the noun-verb
# difference only for words with a sensory experience
# rating of 0:
coef(nv_m2)
p + geom_smooth(method = "lm") +
geom_point(aes(x = 0, y = coef(nv_m2)[1]), size = 6,
shape = 22, fill = "white", show.legend = F)
# from https://www.muscardinus.be/statistics/nested.html
library(tidyverse)
set.seed(123)
n_school <- 10
mean_n_class <- 7
mean_n_student <- 5
n_class <- rpois(n_school, mean_n_class)
schools <- map2_df(
seq_len(n_school), n_class,
~tibble(
school = .x, class = seq_len(.y), students = rpois(.y, mean_n_student)
)
) %>%
group_by(school, class) %>%
do(
student = tibble(student = seq_len(.$students))
) %>%
unnest(student) %>%
mutate(
class2 = interaction(class, school, drop = TRUE),
student2 = interaction(class2, student, drop = TRUE)
)
with(schools, table(class2, school)) %>%
image(
col = grey.colors(10, start = 1, end = 0),  axes = FALSE,  xlab = "Class",
ylab = "School"
)
?image
school_sd <- 2
class_sd <- 2
noise_sd <- 1
intercept <- 50
school_effect <- rnorm(n_school, mean = 0, sd = school_sd)
class_effect <- rnorm(length(levels(schools$class2)), mean = 0, sd = class_sd)
schools <- schools %>%
mutate(
mu = intercept + school_effect[school] + class_effect[class2],
y = mu + rnorm(n(), mean = 0, sd = noise_sd)
)
library(lme4)
lmer(y ~ (1 | school) + (1 | school:class), data = schools)
7+5
7-11
8+5
sapply(10:20, function(i) i-5)
sapply(10:20, function(i) i+115)
sapply(10:20, function(i) i+11)
6+11
6-5
library(tidyverse)
library(lme4)
library(effects)
library(skimr)
library(languageR)
library(broom)
library(afex)
library(MuMIn)
library(lmerTest)
library(lattice)
library(beeswarm)
library(languageR)
library(moderndive)
hulk      <-   tibble(trial = c(1:10), RTs = c(7,6,5,6,4,4,3,3,2,2))
cptmarvel <-   tibble(trial = c(1:10), RTs = c(8,5,6,7,7,7.5,8,7,8,8))
groot     <-   tibble(trial = c(1:10), RTs = c(12,11,13,14,11,9,8,8,9,8))
ironman   <-   tibble(trial = c(1:10), RTs = c(4,5,5,6,7,7,8,9,9,9))
av <- rbind(mutate(hulk, participant = "Hulk"),
mutate(cptmarvel, participant = "Captain Marvel"),
mutate(groot, participant = "Groot"),
mutate(ironman, participant = "Iron Man"))
# plot
(p <- ggplot(av, aes(x = trial, y = RTs)) +
geom_point() + facet_wrap(~ participant))
# fit a simple lm
m_av <- lm(RTs ~ trial, data = av)
p + geom_smooth(method = "lm", se = F)
p + geom_abline(aes(intercept = coef(m_av)[1],
slope = coef(m_av)[2]),
lty = 2, col = "blue")
# fit a model with varying intercept
m_av01 <- lmer(RTs ~ trial + (1 | participant), data = av)
# get coefficients for individual participants:
m_av01_coef <- coef(m_av01)$participant %>% rownames_to_column() %>%
setNames(c("participant", "intercept", "slope"))
# visualize varying intercepts:
left_join(av, m_av01_coef) %>%
ggplot(aes(x = trial, y = RTs)) +
geom_point() + facet_wrap(~ participant) +
geom_abline(aes(intercept = intercept, slope = slope),
col = "blue", lty = 2)
# add varying slopes:
m_av02 <- lmer(RTs ~ trial + (1 + trial | participant), data = av)
# get coefficients:
m_av02_coef <- coef(m_av02)$participant %>% rownames_to_column() %>%
setNames(c("participant", "intercept02", "slope02"))
left_join(left_join(av, m_av01_coef), m_av02_coef) %>%
ggplot(aes(x = trial, y = RTs)) +
geom_point() + facet_wrap(~ participant) +
geom_abline(aes(intercept = intercept, slope = slope),
col = "blue", lty = 2) +
geom_abline(aes(intercept = intercept02, slope = slope02),
col = "red", lty = 3) +
geom_abline(aes(intercept = coef(m_av)[1],
slope = coef(m_av)[2]),
lty = 4, col = "grey")
# load data (from languageR)
data("lexdec")
# remove outliers, only use correct answers
lexdec2 <- lexdec[lexdec$RT < 7, ]
lexdec3 <- lexdec2[lexdec2$Correct == "correct", ]
# center relevant data
lexdec3$Frequency.c <- scale(lexdec3$Frequency, scale = F)
lexdec3$Trial.c <- scale(lexdec3$Trial, scale = F)
# plot each participant in a single panel
xylowess.fnc(RT ~ Trial | Subject, data = lexdec3,
ylabel = "log RT", xlabel = "Trial")
# model: Reaction Time ~ Frequency
m1 <- lme4::lmer(RT ~ Frequency.c +
(1 | Subject) +
(1 | Word),
data = lexdec3,
REML = F) # REML: restricted maximum likelihood
factorial(20)
set.seed(1)
n <- 100
tr <- rbinom(100, 1, 0.5)
y <- 1 + tr + rnorm(n, 0, 3)
tr
y
diff(by(y, tr, mean))
y <- c(6,5,5,3,4,5,6,7)
diff(by(y, tr, mean))
y <- 1 + tr + rnorm(n, 0, 3)
y
str(y)
y <- c(6,5,5,3,4,5,6,7)
str(y)
tr
y <- sample(1:6, 100)
y <- sample(1:6, 100, replace = T)
diff(by(y, tr, mean))
# install a package
!require("tidyerse")
# install a package
!require("tidyverse")
69+28.95
47.5+20.75
library(collostructions)
?collex.distst
?collex.dist
set.seed(123)
library(zipfR)
set.seed(123)
d <- data.frame(cxn = sample(c("A", "B"), 100, replace = T),
freq1 = rnorm(100, mean = 50, sd = 20),
freq2 = rnorm(100, mean = 50, sd = 20))
d
set.seed(123)
d <- data.frame(cxn = sample(c("A", "B"), 100, replace = T),
freq1 = round(rnorm(100, mean = 50, sd = 20)),
freq2 = round(rnorm(100, mean = 50, sd = 20)))
d
collex.dist(d)
subset(collex.dist(d), SIGNIF != "ns")
d2 <- d
d2$freq1 <- d2$freq1*10
d2$freq2 <- d2$freq2*10
subset(collex.dist(d), SIGNIF != "ns")
head(subset(collex.dist(d), SIGNIF != "ns"))
head(subset(collex.dist(d), SIGNIF != "ns"))
head(subset(collex.dist(d2), SIGNIF != "ns"))
head(subset(collex.dist(d), SIGNIF != "ns"))
head(subset(collex.dist(d2), SIGNIF != "ns"))
fion <- readxl::read_xlsx("/Users/stefanhartmann/sciebo/Sciebo Documents/Bilingual_Language_Acquisition/Raw_data_May2020/fion_wordlist.xlsx")
View(fion)
silvie <- readxl::read_xlsx("/Users/stefanhartmann/sciebo/Sciebo Documents/Bilingual_Language_Acquisition/Raw_data_May2020/silvie_wordlist02.xlsx")
View(silvie)
sum(fion$n)
sum(fion$n_CHI)
sum(fion$n_CDS)
sum(silvie$n_CHI)
sum(silvie$n_CDS)
sum(fion$n_CHI)+sum(fion$n_CDS)+sum(silvie$n_CHI)+sum(silvie$n_CDS)
library(tidyverse)
library(readxl)
?readxl::excel_sheets
excel_sheets("/Users/stefanhartmann/sciebo/Sciebo Documents/Bilingual_Language_Acquisition/Raw_data_May2020/fion_wordlist.xlsx")
length(excel_sheets("/Users/stefanhartmann/sciebo/Sciebo Documents/Bilingual_Language_Acquisition/Raw_data_May2020/fion_wordlist.xlsx"))
fion$n_CHI+fion$n_CDS
sum(fion$n_CHI)+sum(fion$n_CDS)
for(i in 1:21) {
fion <- read_xlsx("/Users/stefanhartmann/sciebo/Sciebo Documents/Bilingual_Language_Acquisition/Raw_data_May2020/fion_wordlist.xlsx", sheet = i)
if(i == 1) {
n_all <- sum(fion$n_CHI)+sum(fion$n_CDS)
} else {
n_all <- n_all + sum(fion$n_CHI)+sum(fion$n_CDS)
}
}
length(excel_sheets("/Users/stefanhartmann/sciebo/Sciebo Documents/Bilingual_Language_Acquisition/Raw_data_May2020/silvie_wordlist.xlsx"))
length(excel_sheets("/Users/stefanhartmann/sciebo/Sciebo Documents/Bilingual_Language_Acquisition/Raw_data_May2020/silvie_wordlist02.xlsx"))
for(i in 1:10) {
silvie <- read_xlsx("/Users/stefanhartmann/sciebo/Sciebo Documents/Bilingual_Language_Acquisition/Raw_data_May2020/silvie_wordlist02.xlsx", sheet = i)
if(i == 1) {
n_all <- sum(silvie$n_CHI)+sum(silvie$n_CDS)
} else {
n_all <- n_all + sum(silvie$n_CHI)+sum(silvie$n_CDS)
}
}
library(tidyverse)
library(readxl)
length(excel_sheets("/Users/stefanhartmann/sciebo/Sciebo Documents/Bilingual_Language_Acquisition/Raw_data_May2020/fion_wordlist.xlsx"))
length(excel_sheets("/Users/stefanhartmann/sciebo/Sciebo Documents/Bilingual_Language_Acquisition/Raw_data_May2020/silvie_wordlist02.xlsx"))
for(i in 1:21) {
fion <- read_xlsx("/Users/stefanhartmann/sciebo/Sciebo Documents/Bilingual_Language_Acquisition/Raw_data_May2020/fion_wordlist.xlsx", sheet = i)
if(i == 1) {
n_all <- sum(fion$n_CHI)+sum(fion$n_CDS)
} else {
n_all <- n_all + sum(fion$n_CHI)+sum(fion$n_CDS)
}
}
for(i in 1:10) {
silvie <- read_xlsx("/Users/stefanhartmann/sciebo/Sciebo Documents/Bilingual_Language_Acquisition/Raw_data_May2020/silvie_wordlist02.xlsx", sheet = i)
if(i == 1) {
n_all_silvie <- sum(silvie$n_CHI)+sum(silvie$n_CDS)
} else {
n_all_silvie <- n_all + sum(silvie$n_CHI)+sum(silvie$n_CDS)
}
}
n_all+n_all_silvie
f <- list.files("/Users/stefanhartmann/sciebo/Sciebo Documents/Bilingual_Language_Acquisition/Raw_data_May2020/silvie_wordlist", full.names = T)
f <- list.files("/Users/stefanhartmann/sciebo/Sciebo Documents/Bilingual_Language_Acquisition/Raw_data_May2020/silvie_wordlist", full.names = T)
for(i in 1:length(f)) {
silvie <- read_xlsx(f[i])
if(i == 1) {
n_all_silvie <- sum(silvie$n_CHI)+sum(silvie$n_CDS)
} else {
n_all_silvie <- n_all + sum(silvie$n_CHI)+sum(silvie$n_CDS)
}
}
n_all+n_all_silvie
f <- list.files("/Users/stefanhartmann/sciebo/Sciebo Documents/Bilingual_Language_Acquisition/Raw_data_May2020/lily_wordlist", full.names = T)
for(i in 1:length(f)) {
lily <- read_xlsx(f[i])
if(i == 1) {
n_all_lily <- sum(lily$n_CHI)+sum(lily$n_CDS)
} else {
n_all_lily <- n_all + sum(lily$n_CHI)+sum(lily$n_CDS)
}
}
n_all+n_all_silvie+n_all_lily
install.packages("quanteda")
library(gutenbergr)
gutenberg_languages
filter(gutenberg_languages, language=="de")
library(tidyverse)
gl <- gutenberg_languages
filter(gl, language=="de")
glde <- filter(gl, language=="de")
gutenberg_metadata
gutenberg_metadata
586.2*6
586.2*9
586.2*6
586.2*8
707.62*6
4689.6+4245.72
544.33*4
544.33*5
544.33*6
544.33*4
544.33*5
3000/707.62
707.62*4
707.62*4.5
10+6+2+8+18
# relative Bestehensgrenze:
117.5*0.78
# Punktverteilung
seq(91, 160, length.out = 11)
127+139+148+145+119+112
790/6
0.78*131.66
60+88+69
217/3
library(tidyverse)
d <- read_csv("/Users/stefanhartmann/sciebo/ARENAS WP2/Data DE 12022024/germany.csv")
d$Text
grep("#genderwahn", d)
grep("#genderwahn", d$Text)
grep("genderwahn", d$Text, ignore.case = T)
grep("gender", d$Text, ignore.case = T)
library(writexl)
d[grep("gender", d$Text, ignore.case = T),]
d[grep("gender", d$Text, ignore.case = T),] %>% write_xlsx("gender_twitter.xlsx")
d <- read_csv("/Users/stefanhartmann/sciebo/ARENAS WP2/Data DE 12022024/facebook-de.csv")
d[grep("gender", d$`Post, Text`, ignore.case = T),] %>% write_xlsx("gender_twitter.xlsx")
d <- read_csv("/Users/stefanhartmann/sciebo/ARENAS WP2/Data DE 12022024/germany.csv")
d[grep("gender", d$Text, ignore.case = T),] %>% write_xlsx("gender_twitter.xlsx")
d <- read_csv("/Users/stefanhartmann/sciebo/ARENAS WP2/Data DE 12022024/facebook-de.csv")
d[grep("gender", d$Text, ignore.case = T),] %>% write_xlsx("gender_fb.xlsx")
d[grep("gender", d$`Post, Text`, ignore.case = T),] %>% write_xlsx("gender_fb.xlsx")
d[grep("gender", d$`Post, Text`, ignore.case = T),]
d <- read_csv("/Users/stefanhartmann/sciebo/ARENAS WP2/Data DE 12022024/youtube-GER-clean.csv")
d[grep("gender", d$`Post, Text`, ignore.case = T),] %>% write_xlsx("gender_youtube.xlsx")
d[grep("gender", d$`Comment, Text`, ignore.case = T),] %>% write_xlsx("gender_youtube.xlsx")
d[grep("gender", d$`Comment, Text`, ignore.case = T),]
library(concordances)
setwd("~/sciebo/Projekte/Turin2024/data_and_scripts")
d <- getNSE("view_dewac_20240306183956.xml", xml = TRUE)
View(d)
d <- getNSE("view_dewac_20240306183956.xml", xml = TRUE, context_tags = TRUE)
View(d)
d <- getNSE("view_dewac_20240306184358.txt", xml = FALSE, context_tags = TRUE)
d <- getNSE("view_dewac_20240306184358.txt", xml = FALSE)
?getWACKY
d <- getWACKY("view_dewac_20240306184358.txt", tags = "column")
d <- getWACKY("view_dewac_20240306183956.xml", tags = "column", XML = T)
View(d)
d <- readLines("view_dewac_20240306183956.xml")
